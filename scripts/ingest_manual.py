
"""
Manual ingestion pipeline for the JASP RAG chatbot.

Features:
- Loads PDF text and images.
- Extracts BLIP captions, splits text into chunks.
- Embeds text + captions into Chroma vector DB.

Usage:
    poetry run python -m scripts.ingest_manual


"""



import os, shutil, random, logging
from pathlib import Path
from typing import List, Dict
import torch

from src.utils.logger import setup_logger
from src.utils.ids import source_name_from_path, text_chunk_id, image_id, image_caption_id
from src.ingestion.pdf_loader import load_pdf_with_images, PAGE_OFFSET_DEFAULT
from src.ingestion.captioner import BLIPCaptioner
from src.retrieval.splitter import build_page_docs, hybrid_split
from src.retrieval.embedder import TextEmbedder, ImageEmbedder
from src.retrieval.index import ChromaIndex

# ---- Config (you can move to YAML later) ----
PDF_PATH = "data/raw/Statistical-Analysis-in-JASP-A-guide-for-students-2025.pdf"
OUTPUT_IMG_DIR = "data/processed/images"
INDEX_DIR = "data/indexes/chroma"
PAGE_OFFSET = PAGE_OFFSET_DEFAULT
SKIP_IMAGES = 2
MIN_W, MIN_H = 100, 100
CHUNK_SIZE, CHUNK_OVERLAP = 1000, 100
PREVIEW_SAMPLES = 2
RESET_INDEX = True

def main():
    log = setup_logger()
    random.seed(42); 
    if torch.cuda.is_available():
        torch.manual_seed(42)

    # 1) Extract text & images
    log.info("üì• Loading PDF text + extracting images...")
    docs, images_by_page = load_pdf_with_images(
        path=PDF_PATH,
        output_img_dir=OUTPUT_IMG_DIR,
        page_offset=PAGE_OFFSET,
        skip_first=SKIP_IMAGES,
        min_width=MIN_W,
        min_height=MIN_H,
    )

    # 2) Caption images and enrich page docs
    log.info("üñºÔ∏è Captioning images + enriching pages...")
    captioner = BLIPCaptioner()  # graceful fallback if model not available
    page_docs = build_page_docs(docs, images_by_page, captioner)

    # 3) Split into chunks
    log.info("‚úÇÔ∏è  Hybrid splitting...")
    chunks = hybrid_split(page_docs, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP, preview_samples=PREVIEW_SAMPLES)

    # 4) Embeddings + index
    log.info("üì¶ Initializing embedders and index...")
    text_emb = TextEmbedder("BAAI/bge-small-en")
    img_emb = ImageEmbedder("ViT-B-32", "openai")
    index = ChromaIndex(persist_dir=INDEX_DIR)
    if RESET_INDEX:
        index.reset_collections()

    # 4a) Upsert text chunks
    log.info("üß† Embedding + upserting text chunks...")
    source = source_name_from_path(PDF_PATH)
    docs_batch = [c.page_content for c in chunks]
    metas = [{**c.metadata, "type": "text_chunk"} for c in chunks]
    ids = [text_chunk_id(source, m.get("page", -1), i) for i, m in enumerate(metas)]
    embs = text_emb.encode(docs_batch)
    index.upsert_text_chunks(docs_batch, embs, metas, ids)
    log.info(f"‚úÖ Upserted {len(ids)} text chunks.")

    # 4b) Upsert images (CLIP) + captions also into text index
    log.info("üß© Embedding + upserting images (+captions)...")
    img_docs, img_embs, img_metas, img_ids = [], [], [], []
    cap_docs, cap_embs, cap_metas, cap_ids = [], [], [], []

    all_items = [(p, path) for p, paths in images_by_page.items() for path in paths]
    if all_items:
        paths = [path for _, path in all_items]
        emb_list = img_emb.encode_paths(paths)

        # Iterate to build records (align embeddings)
        k = 0
        for page, path in all_items:
            e = emb_list[k]; k += 1
            # caption (reuse the one already generated by BLIPCaptioner)
            cap = captioner.describe(path)

            meta_img = {"type": "image", "source": source, "page": page, "path": path, "caption": cap}
            img_docs.append(cap if cap else f"Image from page {page}")
            img_embs.append(e)
            img_metas.append(meta_img)
            img_ids.append(image_id(source, page, len(img_ids)))

            # Also store caption as text in the text index
            cap_docs.append(cap)
            cap_metas.append({**meta_img, "type": "image_caption"})
            cap_ids.append(image_caption_id(source, page, len(cap_ids)))
        # Upserts
        index.upsert_images(img_docs, img_embs, img_metas, img_ids)

        # Reuse text index for captions
        cap_embs = text_emb.encode(cap_docs) if cap_docs else []
        if cap_docs:
            index.upsert_text_chunks(cap_docs, cap_embs, cap_metas, cap_ids)

        log.info(f"‚úÖ Upserted {len(img_ids)} images and {len(cap_ids)} captions.")
    else:
        log.info("No images found to embed.")

    log.info("üéâ Manual ingestion finished.")

if __name__ == "__main__":
    main()
