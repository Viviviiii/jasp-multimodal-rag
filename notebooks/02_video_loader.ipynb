{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dde511e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a751483",
   "metadata": {},
   "source": [
    "# fine tuning the video load funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422e1026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "import sys, os\n",
    "\n",
    "# Add project root to sys.path\n",
    "repo_root = os.path.abspath(\"..\")   # assuming notebook is in /notebooks\n",
    "sys.path.append(repo_root)\n",
    "\n",
    "# Verify import works\n",
    "from src.ingestion import video_loader\n",
    "from typing import Dict, Any\n",
    "import yt_dlp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b070ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports + Model:Load required libraries and your embedding model.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Choose embedding model for semantic splitting\n",
    "model = SentenceTransformer(\"BAAI/bge-small-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db3b8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick a test video\n",
    "# Short JASP video for testing\n",
    "url = \"https://www.youtube.com/watch?v=j9w7hEfeIbE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c3930ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched meta data:\n",
      "\n",
      "video_id:j9w7hEfeIbE\n",
      "url:https://www.youtube.com/watch?v=j9w7hEfeIbE\n",
      "title:How to do a One-Way Goodness of Fit Chi-Square in JASP (15-10)\n",
      "description:We learn how to calculate a One-Way Chi-Square goodness of fit test in JASP using the setting for Multinomial Test. For the null hypothesis, we assume that the observed values in one group (the Pigs) do not differ from the values in a comparison population (the Bears) or that there is no difference between the group in their choice of building materials: Straw, Sticks, or Brick and mortar. We create a simple data set in Excel, then open the .CSV dataset in JASP. We conduct the test, interpret the results, and write up the findings in APA style. This is the Pig and Bear Chi Square.\n",
      "\n",
      "Download the Friendly, Free, Flexible, Functional JASP software from the official JASP statistics website: https://jasp-stats.org\n",
      "\n",
      "This video teaches the following commands and techniques in JASP:\n",
      "Importing a CSV into JASP\n",
      "One-Way Chi-Square Goodness of Fit\n",
      "Multinomial Test\n",
      "Cramer’s V effect size\n",
      "\n",
      "This video uses the dataset BuildingPermits.csv and JASP version 12.0\n",
      "\n",
      "Bass Walker - Film Noir by Kevin MacLeod is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/) \n",
      "Source: http://incompetech.com/music/royalty-free/index.html?isrc=USUAN1200071 \n",
      "Artist: http://incompetech.com/ \n",
      "\n",
      "Link to a Google Drive folder with all of the files that I use in the videos including spreadsheets, the Bear Handout, and the BuildingPermits.csv dataset. As I add new files, they will appear here, as well. \n",
      "https://drive.google.com/drive/folders/1n9aCsq5j4dQ6m_sv62ohDI69aol3rW6Q?usp=sharing \n",
      "To download, hover your cursor over the file icon and a blue download icon will appear. You do not need to request access to a file.\n",
      "author:Research By Design\n",
      "publish_date:20200622\n",
      "duration:628\n",
      "chapters:[{'start_time': 0.0, 'title': 'Intro', 'end_time': 78.0}, {'start_time': 78.0, 'title': 'Creating the data set', 'end_time': 169.0}, {'start_time': 169.0, 'title': 'Importing the data', 'end_time': 461.0}, {'start_time': 461.0, 'title': 'Kramers V', 'end_time': 628}]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Video info only (include description as meta data)\n",
    "# ---------------------------\n",
    "def fetch_video_info(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch basic metadata of a YouTube video using yt-dlp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        Full YouTube video URL.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing:\n",
    "        - video_id: str, unique YouTube ID of the video\n",
    "        - url: str, the video URL\n",
    "        - title: str, video title\n",
    "        - description: str, video description text\n",
    "        - author: str, uploader channel name\n",
    "        - publish_date: str, upload date (YYYYMMDD)\n",
    "        - duration: int, video length in seconds\n",
    "        - chapters: list of dicts, chapter info if available\n",
    "    \"\"\"\n",
    "    ydl_opts = {\n",
    "        \"quiet\": True,\n",
    "        \"skip_download\": True,\n",
    "        \"noplaylist\": True,\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(url, download=False)\n",
    "        if not info:\n",
    "            raise RuntimeError(f\"yt-dlp failed for {url}\")\n",
    "\n",
    "        meta = {\n",
    "            \"video_id\": info.get(\"id\"),\n",
    "            \"url\": url,\n",
    "            \"title\": info.get(\"title\"),\n",
    "            \"description\": info.get(\"description\") or \"\",\n",
    "            \"author\": info.get(\"uploader\"),\n",
    "            \"publish_date\": info.get(\"upload_date\"),\n",
    "            \"duration\": info.get(\"duration\"),\n",
    "            \"chapters\": info.get(\"chapters\") or []\n",
    "        }\n",
    "\n",
    "    return meta\n",
    "\n",
    "meta= fetch_video_info(url)\n",
    "\n",
    "print (\"fetched meta data:\\n\")\n",
    "for k,v in meta.items():\n",
    "    print(f\"{k}:{v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c98700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0513b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened text preview:\n",
      "We are now going to calculate a one-way chi-square goodness-of-fit test in JASP. For this example, we are going to assume that the observed values in one group (the Pigs) do not differ from the values in a comparison population (the Bears) or that there is \"no difference\" between the groups. This is ...\n",
      "\n",
      "First 3 segments:\n",
      "{'text': 'We are now going to calculate a one-way', 'start': 0.0, 'duration': 5.64}\n",
      "{'text': 'chi-square goodness-of-fit test in', 'start': 2.85, 'duration': 6.27}\n",
      "{'text': 'JASP. For this example, we are going to', 'start': 5.64, 'duration': 5.49}\n"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "import requests\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def fetch_transcript(url: str, lang: str = \"en\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch transcript (manual or auto) for a YouTube video using yt-dlp.\n",
    "    Returns both raw segments and a flattened text block.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        YouTube video URL.\n",
    "    lang : str\n",
    "        Language code for captions (default: \"en\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "          \"segments\": [ { \"text\": str, \"start\": float, \"duration\": float }, ... ],\n",
    "          \"full_text\": str   # flattened transcript text\n",
    "        }\n",
    "    \"\"\"\n",
    "    ydl_opts = {\n",
    "        \"quiet\": True,\n",
    "        \"skip_download\": True,\n",
    "        \"writesubtitles\": True,\n",
    "        \"subtitleslangs\": [lang],\n",
    "        \"subtitlesformat\": \"json3\"\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(url, download=False)\n",
    "\n",
    "        # check manual or auto captions\n",
    "        subs = info.get(\"subtitles\") or {}\n",
    "        auto_subs = info.get(\"automatic_captions\") or {}\n",
    "        tracks = subs.get(lang) or auto_subs.get(lang)\n",
    "\n",
    "        if not tracks:\n",
    "            print(\"⚠️ No transcript available for this video.\")\n",
    "            return {\"segments\": [], \"full_text\": \"\"}\n",
    "\n",
    "        # pick JSON3 format\n",
    "        sub_url = next((t[\"url\"] for t in tracks if t[\"ext\"] == \"json3\"), None)\n",
    "        if not sub_url:\n",
    "            print(\"⚠️ No JSON3 subtitle track available.\")\n",
    "            return {\"segments\": [], \"full_text\": \"\"}\n",
    "\n",
    "        # fetch JSON3 captions\n",
    "        resp = requests.get(sub_url)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "\n",
    "        segments = []\n",
    "        for evt in data.get(\"events\", []):\n",
    "            if \"segs\" in evt:\n",
    "                text = \"\".join(seg.get(\"utf8\", \"\") for seg in evt[\"segs\"]).strip()\n",
    "                if text:\n",
    "                    segments.append({\n",
    "                        \"text\": text,\n",
    "                        \"start\": evt.get(\"tStartMs\", 0) / 1000.0,\n",
    "                        \"duration\": evt.get(\"dDurationMs\", 0) / 1000.0\n",
    "                    })\n",
    "\n",
    "        # flatten transcript\n",
    "        full_text = \" \".join(seg[\"text\"] for seg in segments)\n",
    "\n",
    "        return {\"segments\": segments, \"full_text\": full_text}\n",
    "\n",
    "\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=j9w7hEfeIbE\"\n",
    "transcript_data = fetch_transcript(url)\n",
    "\n",
    "print(\"Flattened text preview:\")\n",
    "print(transcript_data[\"full_text\"][:300], \"...\")\n",
    "print(\"\\nFirst 3 segments:\")\n",
    "for seg in transcript_data[\"segments\"][:3]:\n",
    "    print(seg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d81bf1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Produced 8 chunks\n",
      "\n",
      "ID: j9w7hEfeIbE_ch0_sem0\n",
      "Chapter: Intro\n",
      "Text:\n",
      " We are now going to calculate a one-way chi-square goodness-of-fit test in JASP. For this example, we are going to assume that the observed values in one group (the Pigs) do not differ from the values in a comparison population (the Bears) or that there is \"no difference\" between the groups. This is the \"Pig and Bear Chi-Square\"! [Music] A house inspector knows that families of three bears choose building materials in a certain pattern. Out of 70 houses built by bear families, 30 were brick-and-mortar, 20 were sticks, and 20 were straw. The house inspector then randomly samples building permits pulled by 70 little pigs who were building houses to determine whether a pattern exists in the materials that the little pigs chose for housing construction. Our research question is: \"Do the pig and bear building permit patterns differ significantly?\" We know the Bears pattern... is the pigs pattern different? In the previous video,\n",
      "================================================================================\n",
      "ID: j9w7hEfeIbE_ch1_sem0\n",
      "Chapter: Creating the data set\n",
      "Text:\n",
      " we created the entire data set from scratch using Excel. If you still have that data set, use that. If you do not, here is how to create it. There were three building materials used for housing construction, so we will call this first variable \"Materials\". And now we can list the three types of materials: straw, sticks, or brick and mortar. JASP will allow us to import these labels and use them as variable names. The second variable we will need is the number of permits. We will call it \"Permits\". The building inspector consults his notes and reports that the values for each type of building material are 16, 21, and 33. So what we have is a scale variable (the counts) for the number of permits and a nominal variable, or factor, which is the type of permits. We need to save this data set to the desktop as a CSV. Call it \"Building Permits\". Let's check the desktop... there it is! Now we are ready to open JASP. Our first\n",
      "================================================================================\n",
      "ID: j9w7hEfeIbE_ch2_sem0_len0\n",
      "Chapter: Importing the data\n",
      "Text:\n",
      " step will be to import the data set that we just created. We start with the main menu icon and select Open -> Computer -> Desktop, and then select \"BuildingPermits.csv\" Click Open. We see that \"Material\" is a nominal text variable - what SPSS calls a String variable. Notice the small \"a\" on the icon in the blue ball. Permits is intended to be a scale variable. And with this, we are ready to do our chi-square test. There are several options for how to do a chi-square test in JASP, all of them in the Frequencies menu, but with the way that we have entered our data, we will use the Multinomial Test. Go to Frequencies -> Multinomial Tests. Now, this may be a little confusing if you're looking for independent and dependent variables, but remember that JASP is built on the R platform, and JASP is using that language. So the \"Factor\" refers to our groups. What are our groups? The groups are the types of materials. Notice the shaded icons that remind us only nominal and ordinal variables can go here. What would be our counts? We can tell from the shaded icons that this must be a scale variable. Remember that our data are totals, so we want JASP to weight each group - or type of building\n",
      "================================================================================\n",
      "ID: j9w7hEfeIbE_ch2_sem0_len1\n",
      "Chapter: Importing the data\n",
      "Text:\n",
      " the shaded icons that this must be a scale variable. Remember that our data are totals, so we want JASP to weight each group - or type of building material - by the counts, which are the number of permits issued for each type of building material. Already we see some results, but... these are not the results that you are looking for! We still need to specify our expected values the expected values are the comparison value that we learned from the Bears to see this more clearly click on descriptives these results are based on test values that assume equal proportions each type of building material was chosen equally but we have expected proportions that were derived from the Bears. For each type of building material, enter the number appropriate to it: 30 for brick and mortar, 20 for sticks, and 20 for straw. Look at the table for Descriptives. Here we see the observed values that we entered, and the expected values which are the comparison values for the comparison group. Notice that none of the cells have expected frequencies less than 5. That is good news and it meets the assumptions for the chi-square test. Notice, too, that you can ask JASP to display proportions instead of\n",
      "================================================================================\n",
      "ID: j9w7hEfeIbE_ch2_sem0_len2\n",
      "Chapter: Importing the data\n",
      "Text:\n",
      " than 5. That is good news and it meets the assumptions for the chi-square test. Notice, too, that you can ask JASP to display proportions instead of counts. That could be useful if the number of expected values was a different number than 70. Let's say you had 300 comparison values from the Bears, for your 70 observations from the pigs. You could still use the proportions from the Bears and get the same chi-Square results. We can also get a nice descriptive plot right here. If you want the 95% confidence intervals, you can ask for them here. Those intervals are already included in our descriptive plot in the form of bars. Returning to the test statistics, we see that our chi-square value is 1.15, and the probability of getting that chi-square if the null hypothesis is true, is 0.563, which is greater than .05, so these observed data from the three little pigs are not statistically significantly different than the expected values from the Bears. There is no difference in the pattern of choices of the building materials. The pigs and the bears choose house building materials in essentially the same ratio. And remember that if you ever need information about what each of these\n",
      "================================================================================\n",
      "ID: j9w7hEfeIbE_ch2_sem0_len3\n",
      "Chapter: Importing the data\n",
      "Text:\n",
      " and the bears choose house building materials in essentially the same ratio. And remember that if you ever need information about what each of these options does you can click on the blue-i to \"show info for analysis.\" [Music]\n",
      "================================================================================\n",
      "ID: j9w7hEfeIbE_ch3_sem0_len0\n",
      "Chapter: Kramers V\n",
      "Text:\n",
      " The measure of effect size for a chi square is called Cramer's V. I cover Cramer's V in detail in another video about effect size for Chi square. It is actually a measure of association. JASP will calculate this for you, if you do your Chi square using the option Frequencies -> Contingency Table. You see here that we have the option for Phi and Cramer's V. I'm sure that this functionality will eventually be added to the multinomial tests, but until then, here's how you calculate it yourself. The formula for Cramer's V is the square root of the Chi square, divided by n, times t, where n is your sample size and t is the smaller of either R - 1, or c - 1, where r is the number of rows, and c is the number of columns. The smallest value that t can assume is 1. Calculate the Cramer's V by plugging in the values from our test. We already know our chi-square value and our sample size was 70. We had three types of building materials, or three rows, but essentially we only had the two columns, so the smaller of those two options would be 2 minus 1. Divide the Chi square by 70, and take the square root... you get a Cramer's V measure of association of 0.13. And here is a sample APA style\n",
      "================================================================================\n",
      "ID: j9w7hEfeIbE_ch3_sem0_len1\n",
      "Chapter: Kramers V\n",
      "Text:\n",
      " minus 1. Divide the Chi square by 70, and take the square root... you get a Cramer's V measure of association of 0.13. And here is a sample APA style write-up for the one way chi-square goodness-of-fit test. We used chi-square to compare the pattern of porcine and ursine building permits. We concluded the pattern of permits pulled by the pigs did not differ statistically significantly from the pattern of permits pulled by the Bears. The Bears know that building with brick and mortar is smart... and the pigs are at least as smart as the average bear. And that is how we calculate a one-way chi-square goodness-of-fit test in JASP [Music]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "import requests\n",
    "from typing import List, Dict, Any\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ---------------------------\n",
    "# Config\n",
    "# ---------------------------\n",
    "MAX_CHARS = 1200        # ~300 tokens\n",
    "OVERLAP_CHARS = 150\n",
    "SEMANTIC_THRESHOLD = 0.70\n",
    "TEXT_MODEL_NAME = \"BAAI/bge-small-en\"\n",
    "\n",
    "embedder = SentenceTransformer(TEXT_MODEL_NAME)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Splitting functions\n",
    "# ---------------------------\n",
    "def length_split(text: str, meta: Dict[str, Any], uid_prefix: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Hard split text into chunks based on length (chars).\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=MAX_CHARS,\n",
    "        chunk_overlap=OVERLAP_CHARS\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "    for i, split_text in enumerate(splitter.split_text(text)):\n",
    "        chunks.append({\n",
    "            \"id\": f\"{uid_prefix}_len{i}\",\n",
    "            \"text\": split_text,\n",
    "            \"meta\": meta\n",
    "        })\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def semantic_split(text: str, meta: Dict[str, Any], uid_prefix: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Semantic split using cosine similarity between sentences.\n",
    "    Falls back to length split if chunks too large.\n",
    "    \"\"\"\n",
    "    if len(text) <= MAX_CHARS:\n",
    "        return [{\"id\": f\"{uid_prefix}_sem0\", \"text\": text, \"meta\": meta}]\n",
    "\n",
    "    sentences = text.split(\". \")\n",
    "    embeddings = embedder.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    sims = util.pytorch_cos_sim(embeddings[:-1], embeddings[1:]).diagonal()\n",
    "    sims = sims.cpu().numpy()\n",
    "\n",
    "    breakpoints = [i+1 for i, score in enumerate(sims) if score < SEMANTIC_THRESHOLD]\n",
    "\n",
    "    chunks, start = [], 0\n",
    "    seg_id = 0\n",
    "    for bp in breakpoints + [len(sentences)]:\n",
    "        chunk_text = \". \".join(sentences[start:bp]).strip()\n",
    "        if chunk_text:\n",
    "            if len(chunk_text) > MAX_CHARS:\n",
    "                # fallback to length split\n",
    "                subchunks = length_split(chunk_text, meta, f\"{uid_prefix}_sem{seg_id}\")\n",
    "                chunks.extend(subchunks)\n",
    "            else:\n",
    "                chunks.append({\n",
    "                    \"id\": f\"{uid_prefix}_sem{seg_id}\",\n",
    "                    \"text\": chunk_text,\n",
    "                    \"meta\": meta\n",
    "                })\n",
    "            seg_id += 1\n",
    "        start = bp\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# ---------------------------\n",
    "# Hybrid split (calls the others)\n",
    "# ---------------------------\n",
    "def hybrid_split(meta: Dict[str, Any], transcript: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Hybrid pipeline:\n",
    "    1. Split by chapters.\n",
    "    2. Semantic split for large blocks.\n",
    "    3. Length fallback for oversized chunks.\n",
    "    \"\"\"\n",
    "    segments = transcript[\"segments\"]\n",
    "    if not segments:\n",
    "        return []\n",
    "\n",
    "    # --- 1. Chapter-based grouping ---\n",
    "    chapter_blocks = []\n",
    "    if meta.get(\"chapters\"):\n",
    "        chapters = meta[\"chapters\"]\n",
    "        for i, ch in enumerate(chapters):\n",
    "            start = ch[\"start_time\"]\n",
    "            end = chapters[i+1][\"start_time\"] if i + 1 < len(chapters) else float(\"inf\")\n",
    "\n",
    "            texts = [s[\"text\"] for s in segments if start <= s[\"start\"] < end]\n",
    "            block_text = \" \".join(texts).strip()\n",
    "            if block_text:\n",
    "                chapter_blocks.append({\n",
    "                    \"text\": block_text,\n",
    "                    \"meta\": {\n",
    "                        \"video_id\": meta[\"video_id\"],\n",
    "                        \"url\": meta[\"url\"],\n",
    "                        \"title\": meta[\"title\"],\n",
    "                        \"chapter\": ch.get(\"title\"),\n",
    "                        \"start_time\": start,\n",
    "                        \"yt_link\": f\"{meta['url']}&t={int(start)}s\",\n",
    "                        \"chapter_index\": i\n",
    "                    }\n",
    "                })\n",
    "    else:\n",
    "        block_text = transcript[\"full_text\"].strip()\n",
    "        chapter_blocks = [{\n",
    "            \"text\": block_text,\n",
    "            \"meta\": {\n",
    "                \"video_id\": meta[\"video_id\"],\n",
    "                \"url\": meta[\"url\"],\n",
    "                \"title\": meta[\"title\"],\n",
    "                \"chapter\": None,\n",
    "                \"start_time\": 0,\n",
    "                \"yt_link\": meta[\"url\"],\n",
    "                \"chapter_index\": 0\n",
    "            }\n",
    "        }]\n",
    "\n",
    "    # --- 2 & 3. Semantic + fallback length split ---\n",
    "    final_chunks = []\n",
    "    for block in chapter_blocks:\n",
    "        uid_prefix = f\"{block['meta']['video_id']}_ch{block['meta']['chapter_index']}\"\n",
    "        chunks = semantic_split(block[\"text\"], block[\"meta\"], uid_prefix)\n",
    "        final_chunks.extend(chunks)\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "# ---------------------------\n",
    "# Example usage\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.youtube.com/watch?v=j9w7hEfeIbE\"\n",
    "    meta = fetch_video_info(url)\n",
    "    transcript = fetch_transcript(url, lang=\"en\")\n",
    "    chunks = hybrid_split(meta, transcript)\n",
    "\n",
    "    print(f\"✅ Produced {len(chunks)} chunks\\n\")\n",
    "    for ch in chunks:\n",
    "        print(\"ID:\", ch[\"id\"])\n",
    "        print(\"Chapter:\", ch[\"meta\"][\"chapter\"])\n",
    "        print(\"Text:\\n\", ch[\"text\"])\n",
    "        print(\"=\" * 80)  # separator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a887759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 39 Max: 232 Avg: 169.0\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(ch[\"text\"].split()) for ch in chunks]\n",
    "print(\"Min:\", min(lengths), \"Max:\", max(lengths), \"Avg:\", sum(lengths)/len(lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faaecb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[201, 217, 284, 241, 249, 49, 291, 157]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-small-en\")\n",
    "token_counts = [len(tokenizer.encode(ch[\"text\"])) for ch in chunks]\n",
    "print(token_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd126b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c03e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6fec4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5400eae",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "enforce_max_chars() got an unexpected keyword argument 'overlap_chars'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m threshold \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m0.6\u001b[39m, \u001b[32m0.7\u001b[39m, \u001b[32m0.8\u001b[39m]:\n\u001b[32m      4\u001b[39m     blocks = semantic_split(transcript, model, threshold=threshold)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     sized = \u001b[43menforce_max_chars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_chars\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlap_chars\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThreshold=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sized)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: enforce_max_chars() got an unexpected keyword argument 'overlap_chars'"
     ]
    }
   ],
   "source": [
    "#Parameter tuning experiments\n",
    "#Wrap tests in loops to experiment systematically.\n",
    "for threshold in [0.6, 0.7, 0.8]:\n",
    "    blocks = semantic_split(transcript, model, threshold=threshold)\n",
    "    sized = enforce_max_chars(blocks, max_chars=1200, overlap_chars=100)\n",
    "    print(f\"Threshold={threshold}: {len(sized)} chunks\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb43e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save experimental outputs (optional):Store results for inspection later.\n",
    "import json\n",
    "\n",
    "with open(\"transcript_blocks.json\", \"w\") as f:\n",
    "    json.dump(sized_blocks, f, indent=2)\n",
    "\n",
    "print(\"Saved experiment results to transcript_blocks.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jasp-multimodal-rag-F1fOabRm-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
