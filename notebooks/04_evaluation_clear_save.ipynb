{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a820c7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-21 16:50:35.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mretrieval.retrieval\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1m✅ Loaded FlagEmbeddingReranker from llama_index.postprocessor.flag_embedding_reranker\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#setup\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "# add /Users/ywxiu/jasp-multimodal-rag/src to sys.path\n",
    "project_root = Path.cwd().parent  # goes from notebooks → jasp-multimodal-rag\n",
    "src_path = project_root / \"src\"\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "import retrieval.retrieval as retr  # adjust if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7555a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_config(cfg: dict, test_set: list):\n",
    "    \"\"\"Run retrieval for a given config and compute H1a, H1b, H2.\"\"\"\n",
    "    # You must have apply_config(cfg) and retr.retrieve_top_k defined somewhere\n",
    "    apply_config(cfg)\n",
    "\n",
    "    retrieval_results = []\n",
    "\n",
    "    for item in test_set:\n",
    "        q = item[\"query\"]\n",
    "        qid = item[\"id\"]\n",
    "        relevant_ids = set(item[\"relevant_chunk_ids\"])\n",
    "\n",
    "        # Run your pipeline\n",
    "        results = retr.retrieve_top_k(q, top_k=cfg[\"TOP_FINAL\"])\n",
    "\n",
    "        retrieved_ids = [\n",
    "            # try doc_id, fall back to section_id\n",
    "            (getattr(r, \"metadata\", {}) or {}).get(\"doc_id\")\n",
    "            or (getattr(r, \"metadata\", {}) or {}).get(\"section_id\")\n",
    "            for r in results\n",
    "        ]\n",
    "\n",
    "        success_at_k = any(rid in relevant_ids for rid in retrieved_ids[:cfg[\"TOP_FINAL\"]])\n",
    "        top1_relevant = retrieved_ids[0] in relevant_ids if retrieved_ids else False\n",
    "\n",
    "        retrieval_results.append({\n",
    "            \"id\": qid,\n",
    "            \"query\": q,\n",
    "            \"answerable\": item[\"answerable\"],\n",
    "            \"retrieved_ids\": retrieved_ids,\n",
    "            \"success_at_k\": success_at_k,\n",
    "            \"top1_relevant\": top1_relevant,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(retrieval_results)\n",
    "    df_answerable   = df[df[\"answerable\"] == True]\n",
    "    df_unanswerable = df[df[\"answerable\"] == False]\n",
    "\n",
    "    H1a = df_answerable[\"success_at_k\"].mean() if not df_answerable.empty else float(\"nan\")\n",
    "    H1b = df_answerable[\"top1_relevant\"].mean() if not df_answerable.empty else float(\"nan\")\n",
    "\n",
    "    if not df_unanswerable.empty:\n",
    "        is_empty = (df_unanswerable[\"retrieved_ids\"].str.len() == 0)\n",
    "        H2 = is_empty.mean()\n",
    "    else:\n",
    "        H2 = float(\"nan\")\n",
    "\n",
    "    return H1a, H1b, H2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feb836d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'query', 'answerable', 'ground_truth_answer', 'relevant_chunk_ids']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1 – Load test_set\n",
    "TEST_JSON = Path(\"/Users/ywxiu/jasp-multimodal-rag/data/test_QA/QA_filled_1.json\")\n",
    "\n",
    "with open(TEST_JSON, \"r\") as f:\n",
    "    test_set = json.load(f)\n",
    "\n",
    "\n",
    "print(list(test_set[0]))\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac4009ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 – Helper to apply a config to the retrieval module\n",
    "def apply_config(cfg: dict):\n",
    "    retr.K_BM25        = cfg[\"K_BM25\"]\n",
    "    retr.BOOST_WEIGHT  = cfg[\"boost_weight\"]\n",
    "    retr.K_SEMANTIC    = cfg[\"K_SEMANTIC\"]\n",
    "    retr.RRF_K         = cfg[\"RRF_K\"]\n",
    "    retr.TOP_AFTER_RRF = cfg[\"TOP_AFTER_RRF\"]\n",
    "    retr.SCORE_THRESHOLD = cfg[\"score_threshold\"]\n",
    "    retr.TOP_FINAL     = cfg[\"TOP_FINAL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "468095ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import hashlib\n",
    "import traceback\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def cfg_hash(cfg: dict) -> str:\n",
    "    \"\"\"\n",
    "    Stable unique hash for a configuration.\n",
    "    \"\"\"\n",
    "    s = json.dumps(cfg, sort_keys=True)\n",
    "    return hashlib.md5(s.encode()).hexdigest()\n",
    "\n",
    "\n",
    "def load_completed_hashes(save_path: str) -> set:\n",
    "    \"\"\"\n",
    "    Load already completed config hashes from an existing results CSV.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_path):\n",
    "        return set()\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(save_path)\n",
    "        if \"config_hash\" not in df.columns:\n",
    "            print(\"⚠️ Existing file has no 'config_hash' column, treating as empty.\")\n",
    "            return set()\n",
    "        return set(df[\"config_hash\"].astype(str).tolist())\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Warning: Could not read existing results file ({e}). Restarting fresh.\")\n",
    "        return set()\n",
    "\n",
    "\n",
    "def append_result_row(save_path: str, row: dict, header_written: bool):\n",
    "    \"\"\"\n",
    "    Append a single result row to CSV immediately.\n",
    "    Ensures nothing is lost even if the process stops.\n",
    "    \"\"\"\n",
    "    write_header = not header_written or not os.path.exists(save_path)\n",
    "\n",
    "    with open(save_path, \"a\", newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=row.keys())\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def build_all_configs(param_space: dict):\n",
    "    \"\"\"\n",
    "    Build list of (cfg, config_hash) pairs from param_space.\n",
    "    cfg does NOT contain config_hash, so it's safe to pass to evaluate_config.\n",
    "    \"\"\"\n",
    "    param_items = list(param_space.items())\n",
    "    configs = []\n",
    "\n",
    "    for values in product(*[vals for _, vals in param_items]):\n",
    "        cfg = {param_items[i][0]: values[i] for i in range(len(values))}\n",
    "        h = cfg_hash(cfg)\n",
    "        configs.append((cfg, h))\n",
    "\n",
    "    return configs\n",
    "\n",
    "\n",
    "def run_full_grid_search_resumable(param_space, test_set, save_path=\"grid_search_results.csv\"):\n",
    "    \"\"\"\n",
    "    Resumable grid search:\n",
    "    - Writes each iteration immediately to disk\n",
    "    - Detects already-finished configs (via config_hash column)\n",
    "    - Continues where you left off\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Load previously completed hashes\n",
    "    completed = load_completed_hashes(save_path)\n",
    "    print(f\"Found {len(completed)} previously completed configurations.\")\n",
    "\n",
    "    # 2) Build full configuration list\n",
    "    configs = build_all_configs(param_space)\n",
    "    print(f\"Total combinations: {len(configs)}\")\n",
    "\n",
    "    # 3) Loop through configs\n",
    "    header_written = os.path.exists(save_path)\n",
    "\n",
    "    for cfg, h in tqdm(configs, desc=\"Grid Search\"):\n",
    "\n",
    "        # Skip if already completed\n",
    "        if h in completed:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Evaluate this config (cfg does NOT contain config_hash)\n",
    "            H1a, H1b, H2 = evaluate_config(cfg, test_set)\n",
    "\n",
    "            # Prepare row for CSV:\n",
    "            #   - all hyperparameters (cfg)\n",
    "            #   - config_hash\n",
    "            #   - metrics\n",
    "            row = {\n",
    "                **cfg,\n",
    "                \"config_hash\": h,\n",
    "                \"H1a\": H1a,\n",
    "                \"H1b\": H1b,\n",
    "                \"H2\": H2,\n",
    "            }\n",
    "\n",
    "            # Save immediately\n",
    "            append_result_row(save_path, row, header_written)\n",
    "            header_written = True\n",
    "            completed.add(h)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error evaluating config {cfg} (hash={h}): {e}\")\n",
    "            traceback.print_exc()\n",
    "            # Continue with next config even if this one fails\n",
    "\n",
    "    print(f\"✅ Finished. Results saved to {save_path}\")\n",
    "    return pd.read_csv(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64de980f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current param_space:\n",
      "  K_BM25: [10]\n",
      "  K_SEMANTIC: [10]\n",
      "  RRF_K: [120]\n",
      "  TOP_AFTER_RRF: [10]\n",
      "  TOP_FINAL: [5]\n",
      "  score_threshold: [-3, -2, -1, -0.5, 0, 0.5, 1, 1.5, 2.0]\n",
      "  boost_weight: [0.0, 1.0, 3.0, 4.0, 5.0]\n",
      "Total combinations: 45\n",
      "First (and only?) config: {'K_BM25': 10, 'K_SEMANTIC': 10, 'RRF_K': 120, 'TOP_AFTER_RRF': 10, 'TOP_FINAL': 5, 'score_threshold': -3, 'boost_weight': 0.0}\n"
     ]
    }
   ],
   "source": [
    "#Step 5 – Parameter space for tuning\n",
    "param_space = {\n",
    "    \"K_BM25\":          [10],\n",
    "    \"K_SEMANTIC\":      [10],\n",
    "    \"RRF_K\":           [120],\n",
    "    \"TOP_AFTER_RRF\":   [10],\n",
    "    \"TOP_FINAL\":       [5],\n",
    "    \"score_threshold\": [-3, -2,-1, -0.5, 0, 0.5, 1, 1.5, 2.0],\n",
    "    \"boost_weight\":    [0.0, 1.0, 3.0, 4.0,5.0],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "print(\"Current param_space:\")\n",
    "for k, v in param_space.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "param_items = list(param_space.items())\n",
    "configs = []\n",
    "for values in product(*[vals for _, vals in param_items]):\n",
    "    cfg = {param_items[i][0]: values[i] for i in range(len(values))}\n",
    "    configs.append(cfg)\n",
    "\n",
    "print(\"Total combinations:\", len(configs))\n",
    "print(\"First (and only?) config:\", configs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60260be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = run_full_grid_search_resumable(\n",
    "    param_space=param_space,\n",
    "    test_set=test_set,\n",
    "    save_path=\"grid_search_results.csv\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jasp-multimodal-rag-F1fOabRm-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
