{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd617ece",
   "metadata": {},
   "source": [
    "#Similarity & Reranking Playground\n",
    "\n",
    "1.Build the pipeline step by step.\n",
    "\n",
    "2.Experiment with dense, sparse, hybrid retrieval.\n",
    "\n",
    "3.Add a CrossEncoder reranker.\n",
    "\n",
    "4.Package the reusable logic into clean functions you can later copy into query.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "834d9a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 ‚Äî Setup & imports\n",
    "\n",
    "#Purpose: load essentials, set logging, and make BM25/Ensemble imports robust across LangChain versions.\n",
    "# set up\n",
    "import sys, os\n",
    "\n",
    "# Add project root to sys.path\n",
    "repo_root = os.path.abspath(\"..\")   # assuming notebook is in /notebooks\n",
    "sys.path.append(repo_root)\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "# Core\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Robust imports for BM25 + Ensemble (LangChain moved some modules across versions)\n",
    "try:\n",
    "    from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "except Exception:\n",
    "    from langchain_community.retrievers import BM25Retriever\n",
    "    from langchain.retrievers import EnsembleRetriever  # still here in many versions\n",
    "\n",
    "# Reranker\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Your ingestion & index\n",
    "import importlib\n",
    "import src.ingestion.video_loader as video_loader\n",
    "from src.retrieval.index import TextIndexer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f6d64ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 17:38:28,359 [INFO] Use pytorch device_name: mps\n",
      "2025-09-29 17:38:28,360 [INFO] Load pretrained SentenceTransformer: BAAI/bge-small-en\n"
     ]
    }
   ],
   "source": [
    "#2 ‚Äî Reload your local module (avoid stale imports)\n",
    "\n",
    "#Purpose: Jupyter caches modules; this forces it to pick up your latest video_loader.py.\n",
    "\n",
    "importlib.reload(video_loader)\n",
    "from src.ingestion.video_loader import fetch_video_info, fetch_transcript, build_docs_from_video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "383ff09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 ‚Äî Config\n",
    "\n",
    "#Purpose: put constants in one place; easy to tweak.\n",
    "\n",
    "URL = \"https://www.youtube.com/watch?v=j9w7hEfeIbE\"\n",
    "CHROMA_DIR = \"data/chroma\"\n",
    "MODEL_NAME = \"BAAI/bge-small-en\"\n",
    "\n",
    "# Retrieval knobs\n",
    "DENSE_K = 5\n",
    "SPARSE_K = 5\n",
    "HYBRID_WEIGHTS = [0.7, 0.3]   # [dense, sparse]\n",
    "RERANK_TOP_K = 5              # final results after reranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f23aa9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 17:38:31,160 [INFO] üì∫ Fetching video info for https://www.youtube.com/watch?v=j9w7hEfeIbE\n",
      "2025-09-29 17:38:33,251 [INFO] ‚úÖ Metadata fetched for video j9w7hEfeIbE (How to do a One-Way Goodness of Fit Chi-Square in JASP (15-10))\n",
      "2025-09-29 17:38:33,252 [INFO] üìù Fetching transcript for https://www.youtube.com/watch?v=j9w7hEfeIbE (lang=en)\n",
      "2025-09-29 17:38:35,762 [INFO] ‚úÖ 186 transcript segments fetched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ID: j9w7hEfeIbE\n",
      "Title   : How to do a One-Way Goodness of Fit Chi-Square in JASP (15-10)\n",
      "#Segments (transcript): 186\n",
      "Has chapters?  True\n"
     ]
    }
   ],
   "source": [
    "#4 ‚Äî Fetch video data\n",
    "\n",
    "#Purpose: get metadata + transcript with helpful prints.\n",
    "meta = fetch_video_info(URL)\n",
    "transcript = fetch_transcript(URL, lang=\"en\")\n",
    "\n",
    "print(\"Video ID:\", meta[\"video_id\"])\n",
    "print(\"Title   :\", meta[\"title\"])\n",
    "print(\"#Segments (transcript):\", len(transcript.get(\"segments\", [])))\n",
    "print(\"Has chapters? \", bool(meta.get(\"chapters\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3e658500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 17:38:35,770 [INFO] ‚úÖ Produced 14 description chunks.\n",
      "2025-09-29 17:38:35,774 [INFO] ‚è© Splitting transcript by 4 chapters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d4dcbdd6e3495190aae4e543a7c3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8ad6c1cc494a7193a0449effc8c943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 17:38:38,338 [INFO] ‚úÖ Produced 8 transcript chunks.\n",
      "2025-09-29 17:38:38,339 [INFO] üì¶ Built 22 docs (desc=14, trans=8).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prepared 22 documents\n",
      "First doc: description | We learn how to calculate a One-Way Chi-Square goodness of fit test in JASP using the setting for Multinomial Test. For the null hypothesis, we assume that the observed values in o ...\n"
     ]
    }
   ],
   "source": [
    "#5 ‚Äî Build documents (description + transcript)\n",
    "\n",
    "#Purpose: convert to query-ready Documents.\n",
    "\n",
    "docs = build_docs_from_video(meta, transcript)\n",
    "print(f\"‚úÖ Prepared {len(docs)} documents\")\n",
    "print(\"First doc:\", docs[0].metadata.get(\"type\"), \"|\", docs[0].page_content[:180], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "42f6a926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: {'description': 14, 'transcript': 8}\n",
      "Description: 14 chunks | avg 115 chars | min 16 | max 587\n",
      "Transcript : 8 chunks | avg 939 chars | min 226 | max 1197\n"
     ]
    }
   ],
   "source": [
    "#Quick dataset stats (chunk counts & sizes)\n",
    "\n",
    "#Purpose: sanity-check your splits and sizes by type.\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter(d.metadata[\"type\"] for d in docs)\n",
    "lens = {\"description\": [], \"transcript\": []}\n",
    "for d in docs:\n",
    "    lens[d.metadata[\"type\"]].append(len(d.page_content))\n",
    "\n",
    "def summarize(kind):\n",
    "    arr = lens.get(kind, [])\n",
    "    if not arr: return \"0 chunks\"\n",
    "    return f\"{len(arr)} chunks | avg {sum(arr)//len(arr)} chars | min {min(arr)} | max {max(arr)}\"\n",
    "\n",
    "print(\"Counts:\", dict(counts))\n",
    "print(\"Description:\", summarize(\"description\"))\n",
    "print(\"Transcript :\", summarize(\"transcript\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5d12fa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 17:38:38,413 [INFO] Use pytorch device_name: mps\n",
      "2025-09-29 17:38:38,413 [INFO] Load pretrained SentenceTransformer: BAAI/bge-small-en\n",
      "2025-09-29 17:38:40,549 [INFO] Use pytorch device_name: mps\n",
      "2025-09-29 17:38:40,549 [INFO] Load pretrained SentenceTransformer: BAAI/bge-small-en\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 17:38:43,123 [INFO] ‚úÖ Inserted 22 docs into Chroma collection 'jasp_text_j9w7hEfeIbE'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dense retriever ready: jasp_text_j9w7hEfeIbE\n"
     ]
    }
   ],
   "source": [
    "#7-Build dense index (Chroma)\n",
    "\n",
    "#Purpose: embed + persist; use a collection name tied to this video id to avoid collisions.\n",
    "\n",
    "import importlib\n",
    "import src.retrieval.index as index_module\n",
    "importlib.reload(index_module)\n",
    "from src.retrieval.index import TextIndexer\n",
    "\n",
    "# now test\n",
    "indexer = TextIndexer(CHROMA_DIR, \"test_collection\", MODEL_NAME)\n",
    "print(hasattr(indexer, \"as_retriever\"))  # should be True\n",
    "\n",
    "\n",
    "# Build a collection name tied to this video ID to avoid mixing across videos\n",
    "collection_name = f\"jasp_text_{meta['video_id']}\"\n",
    "\n",
    "# Initialize your indexer\n",
    "indexer = TextIndexer(\n",
    "    chroma_dir=CHROMA_DIR,   # parameter name matches your class\n",
    "    collection_name=collection_name,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "# Insert docs (upsert)\n",
    "indexer.upsert_documents(docs, source_prefix=collection_name)\n",
    "\n",
    "# Create a LangChain retriever wrapper (needed for hybrid retrieval)\n",
    "dense_retriever = indexer.as_retriever(search_kwargs={\"k\": DENSE_K})\n",
    "\n",
    "print(\"‚úÖ Dense retriever ready:\", collection_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "446c59f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BM25 retriever ready (k = 5 )\n"
     ]
    }
   ],
   "source": [
    "#8 ‚Äî Build BM25 (sparse) retriever\n",
    "\n",
    "#Purpose: exact keyword matching, great for rare terms like ‚ÄúANOVA‚Äù.\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = SPARSE_K\n",
    "print(\"‚úÖ BM25 retriever ready (k =\", SPARSE_K, \")\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5615f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hybrid retriever ready with weights [0.7, 0.3]\n"
     ]
    }
   ],
   "source": [
    "#9 ‚Äî Hybrid retriever (dense + sparse)\n",
    "\n",
    "#Purpose: combine strengths of semantics + keywords.\n",
    "\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[dense_retriever, bm25_retriever],\n",
    "    weights=HYBRID_WEIGHTS\n",
    ")\n",
    "print(\"‚úÖ Hybrid retriever ready with weights\", HYBRID_WEIGHTS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jasp-multimodal-rag-F1fOabRm-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
